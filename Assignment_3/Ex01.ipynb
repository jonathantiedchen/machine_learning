{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 01\n",
    "In this question, you will use the Fashion MNIST dataset and build a custom model using custom training loops (using a different optimizer with a different learning rate for the upper layers and the lower layers) to tackle image classification in the Fashion MNIST dataset.\n",
    "For example, you can use Sequential model [1] from keras to build your custom model. Please note that Sequential model is suitable for a simple stack of layers with the restriction that each layer can only support exactly one input tensor and one output tensor [1]. Alternatively, you can also use Keras Functional API [2], which allows you to create models that are more flexible than the models created using Sequential model [1]. Some hints are as follows:\n",
    "1. Only use five epochs and 32 as batch size.\n",
    "2. Only use softmax and ReLU activation functions.\n",
    "3. Use SGD as the lower optimizer with the learning rate of 1e-4 and Nadam as upper optimizer with a learning rate as 1e-3.\n",
    "4. Use Nadam optimizer [3] from Keras and also use sparse categorical cross entropy as a loss function.\n",
    "5. Display the mean training loss and the mean accuracy over each epoch (updated at each iteration). Also display, validation loss, and accuracy at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "import tensorflow_addons as tfa #for tfa to work, a compatible version of tensorflow has to be installed: check https://github.com/tensorflow/addons\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#to make this notebookâ€™s output stable across runs\n",
    "np.random.seed(42) \n",
    "tf.random.set_seed(42)\n",
    "\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data() \n",
    "X_train_full = X_train_full.astype(np.float32) / 255.\n",
    "X_valid , X_train = X_train_full [:5000] , X_train_full [5000:]\n",
    "y_valid , y_train = y_train_full [:5000] , y_train_full [5000:]\n",
    "X_test = X_test.astype(np.float32) / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define sequential model\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(64, activation='softmax'),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "#define optimizers used in different layers of the model\n",
    "#legacy used to run more efficient on M1/M2 Macs as suggested by warning\n",
    "#WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
    "optimizers = [\n",
    "    tf.keras.optimizers.legacy.SGD(learning_rate=1e-4), #lower optimizer -> close to input\n",
    "    tf.keras.optimizers.legacy.Adam(learning_rate=1e-2) #lower optimizer -> close to output\n",
    "]\n",
    "\n",
    "#assign optimizers to the layers\n",
    "optimizers_and_layers = [(optimizers[0], model.layers[0]), (optimizers[1], model.layers[1:])]\n",
    "optimizer = tfa.optimizers.MultiOptimizer(optimizers_and_layers)\n",
    "\n",
    "#compile with .SparseCategoricalCrossentropy as loss funciton and accuracy as metric (will be later be outputted for every epoch)\n",
    "model.compile(optimizer=optimizer, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.9424 - accuracy: 0.5560\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.9381 - accuracy: 0.5542\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.9241 - accuracy: 0.5528\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.9321 - accuracy: 0.5595\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.9473 - accuracy: 0.5589\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2a1a4f3a0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the model, default batch size is 32, see https://keras.io/api/models/model_training_apis/\n",
    "model.fit(X_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - loss: 1.0123 - accuracy: 0.5223 - 767ms/epoch - 2ms/step\n",
      "\n",
      "Test accuracy: 0.5223000049591064\n"
     ]
    }
   ],
   "source": [
    "#check accuracy of model on the test data\n",
    "test_loss, test_acc = model.evaluate(X_test,  y_test, verbose=2)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
