{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 01\n",
    "In this question, you will use the Fashion MNIST dataset and build a custom model using custom training loops (using a different optimizer with a different learning rate for the upper layers and the lower layers) to tackle image classification in the Fashion MNIST dataset.\n",
    "For example, you can use Sequential model [1] from keras to build your custom model. Please note that Sequential model is suitable for a simple stack of layers with the restriction that each layer can only support exactly one input tensor and one output tensor [1]. Alternatively, you can also use Keras Functional API [2], which allows you to create models that are more flexible than the models created using Sequential model [1]. Some hints are as follows:\n",
    "1. Only use five epochs and 32 as batch size.\n",
    "2. Only use softmax and ReLU activation functions.\n",
    "3. Use SGD as the lower optimizer with the learning rate of 1e-4 and Nadam as upper optimizer with a learning rate as 1e-3.\n",
    "4. Use Nadam optimizer [3] from Keras and also use sparse categorical cross entropy as a loss function.\n",
    "5. Display the mean training loss and the mean accuracy over each epoch (updated at each iteration). Also display, validation loss, and accuracy at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "import tensorflow_addons as tfa #for tfa to work, a compatible version of tensorflow has to be installed: check https://github.com/tensorflow/addons\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#to make this notebookâ€™s output stable across runs\n",
    "np.random.seed(42) \n",
    "tf.random.set_seed(42)\n",
    "\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data() \n",
    "X_train_full = X_train_full.astype(np.float32) / 255.\n",
    "X_valid , X_train = X_train_full [:5000] , X_train_full [5000:]\n",
    "y_valid , y_train = y_train_full [:5000] , y_train_full [5000:]\n",
    "X_test = X_test.astype(np.float32) / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define sequential model\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(64, activation='softmax'),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "#define optimizers used in different layers of the model\n",
    "#legacy used to run more efficient on M1/M2 Macs as suggested by warning\n",
    "#WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
    "optimizers = [\n",
    "    tf.keras.optimizers.legacy.SGD(learning_rate=1e-4), #lower optimizer -> close to input\n",
    "    tf.keras.optimizers.legacy.Adam(learning_rate=1e-2) #lower optimizer -> close to output\n",
    "]\n",
    "\n",
    "#assign optimizers to the layers\n",
    "optimizers_and_layers = [(optimizers[0], model.layers[0]), (optimizers[1], model.layers[1:])]\n",
    "optimizer = tfa.optimizers.MultiOptimizer(optimizers_and_layers)\n",
    "\n",
    "#compile with .SparseCategoricalCrossentropy as loss funciton and accuracy as metric (will be later be outputted for every epoch)\n",
    "model.compile(optimizer=optimizer, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 1.2836 - accuracy: 0.4509\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 1.0840 - accuracy: 0.5367\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 1.0445 - accuracy: 0.5487\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 1.0021 - accuracy: 0.5823\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.9534 - accuracy: 0.6113\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x29c088d30>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the model\n",
    "model.fit(X_train, y_train, epochs=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
