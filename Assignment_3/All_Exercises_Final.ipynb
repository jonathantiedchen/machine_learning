{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "import tensorflow_addons as tfa #for tfa to work, a compatible version of tensorflow has to be installed: check https://github.com/tensorflow/addons\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "#to make this notebookâ€™s output stable across runs\n",
    "np.random.seed(42) \n",
    "tf.random.set_seed(42)\n",
    "\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data() \n",
    "X_train_full = X_train_full.astype(np.float32) / 255.\n",
    "X_valid , X_train = X_train_full [:5000] , X_train_full [5000:]\n",
    "y_valid , y_train = y_train_full [:5000] , y_train_full [5000:]\n",
    "X_test = X_test.astype(np.float32) / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define sequential model\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(64, activation='relu'),\n",
    "  tf.keras.layers.Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "#define optimizers used in different layers of the model\n",
    "#legacy used to run more efficient on M1/M2 Macs as suggested by warning\n",
    "#WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
    "optimizers = [\n",
    "    tf.keras.optimizers.legacy.SGD(learning_rate=1e-4), #lower optimizer -> close to input\n",
    "    tf.keras.optimizers.legacy.Adam(learning_rate=1e-2) #lower optimizer -> close to output\n",
    "]\n",
    "\n",
    "#assign optimizers to the layers\n",
    "optimizers_and_layers = [(optimizers[0], model.layers[0]), (optimizers[1], model.layers[1:])]\n",
    "optimizer = tfa.optimizers.MultiOptimizer(optimizers_and_layers)\n",
    "\n",
    "#compile with .SparseCategoricalCrossentropy as loss funciton and accuracy as metric (will be later be outputted for every epoch)\n",
    "model.compile(optimizer=optimizer, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the model, default batch size is 32, see https://keras.io/api/models/model_training_apis/\n",
    "model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check accuracy of model on the test data\n",
    "test_loss, test_acc = model.evaluate(X_test,  y_test, verbose=2)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions to plot the predictions from lecture exercise\n",
    "\n",
    "#use classnames from lecture example\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "def plot_image(i, predictions_array, true_label, img, label):\n",
    "  true_label, img = label[i], img[i]\n",
    "  plt.grid(False)\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "\n",
    "  plt.imshow(img, cmap=plt.cm.binary)\n",
    "\n",
    "  predicted_label = np.argmax(predictions_array)\n",
    "  if predicted_label == true_label:\n",
    "    color = 'blue'\n",
    "  else:\n",
    "    color = 'red'\n",
    "\n",
    "  plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n",
    "                                100*np.max(predictions_array),\n",
    "                                class_names[true_label]))\n",
    "                   \n",
    "def plot_value_array(i, predictions_array, true_label):\n",
    "  true_label = true_label[i]\n",
    "  plt.grid(False)\n",
    "  plt.xticks(range(10))\n",
    "  plt.yticks([])\n",
    "  thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n",
    "  plt.ylim([0, 1])\n",
    "  predicted_label = np.argmax(predictions_array)\n",
    "\n",
    "  thisplot[predicted_label].set_color('red')\n",
    "  thisplot[true_label].set_color('blue')\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make predictions\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Randomly choose 10 indexes from X_test\n",
    "num_images = 10\n",
    "indexes = np.random.choice(len(X_test), size=num_images, replace=False)\n",
    "test_images = X_test[indexes]\n",
    "label = y_test[indexes]\n",
    "pred = model.predict(test_images)\n",
    "\n",
    "for i in range(num_images):\n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.subplot(1,2,1)\n",
    "    plot_image(i, pred[i], y_test, test_images, label)\n",
    "    plt.subplot(1,2,2)\n",
    "    plot_value_array(i, pred[i],  y_test)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Embedding\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mauricebaier/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#downloading NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#loading datasets\n",
    "fake = pd.read_csv('Fake.csv')\n",
    "true = pd.read_csv('True.csv')\n",
    "\n",
    "#adding label column to the datasets\n",
    "fake['label'] = 1  # Fake news label\n",
    "true['label'] = 0  # True news label\n",
    "\n",
    "#combining datasets\n",
    "data = pd.concat([fake, true], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def text_preprocessing(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    new_text = []\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    for w in tokens:\n",
    "        if w not in stopwords.words('english') and len(w) > 2:\n",
    "            stem_token = ps.stem(w)   \n",
    "            new_text.append(stem_token)  # Add the stemmed word to new_tokens\n",
    "    \n",
    "    text_prep = ' '.join(new_text)  # Join stemmed tokens into a string\n",
    "    return text_prep\n",
    "\n",
    "fake['text'] = fake['text'].apply(text_preprocessing)\n",
    "true['text'] = true['text'].apply(text_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizing text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data['text'])\n",
    "X_sequences = tokenizer.texts_to_sequences(data['text'])\n",
    "max_len = max([len(seq) for seq in X_sequences])\n",
    "X_padded = pad_sequences(X_sequences, maxlen=max_len)\n",
    "\n",
    "#splitting data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, data['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "#defining Bi-LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100))\n",
    "model.add(Bidirectional(LSTM(units=64, activation='relu')))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "#compiling the model\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#training the model\n",
    "model.fit(X_train, y_train, epochs=2, batch_size=64, validation_split=0.2)\n",
    "\n",
    "#evaluating the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Model Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
