{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DISCLAIMER: This Notebook can not be executed as it need to connect to the Azure Storage Account used to store the images. It solely serves as demonstraion how the .npy files were created by extracting the file from Azure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook reads the csv files, which specify the image paths and labels of each image. The functions the access the Image in Azure via the Filepath, download it, convert it to the (224,224,3) shape and store it in an array. The Output of this functions are three np-Arrays, train, test and validation. Each arrays contains the according image data. The arrays are further used in the models. To test if the code of the DL models run, mini datasets were created which are subsets of 50 pictures each. This is done as the files outputted by this function are over 7.5GB and thus cannot be submitted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install required packages specified in requirements file\n",
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "from azure.storage.blob import BlobServiceClient, ContainerClient\n",
    "from azure.core.exceptions import ResourceNotFoundError\n",
    "from PIL import Image\n",
    "import ast\n",
    "import time\n",
    "from keras.models import load_model\n",
    "import tempfile\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up storage\n",
    "connection_string = \"<key>\"\n",
    "container_name = \"meterml\"\n",
    "\n",
    "#create client\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 224\n",
    "channels = 3\n",
    "\n",
    "def load_image(image_path):\n",
    "    \"\"\"Fetches image data from Azure Blob Storage.\"\"\"\n",
    "    try:\n",
    "        # Create a blob service client\n",
    "        blob_service_client = BlobServiceClient(account_url=\"https://<account_name>.blob.core.windows.net\", credential=\"<account_key>\")\n",
    "        blob_client = container_client.get_blob_client(image_path)\n",
    "\n",
    "        # Download the blob's contents as bytes\n",
    "        blob_data = blob_client.download_blob().readall()\n",
    "        \n",
    "        return blob_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching image from Azure: {e}\")\n",
    "        # Return a default image or handle the error appropriately\n",
    "        return tf.zeros((image_size, image_size, channels), dtype=tf.uint8).numpy()\n",
    "\n",
    "def load_and_preprocess_image(path):\n",
    "    \"\"\"Loads an image, decodes it to grayscale, resizes, and normalizes it.\"\"\"\n",
    "    # Load image\n",
    "    image_file = load_image(path)\n",
    "    # Decode the image to grayscale\n",
    "    image_tensor = tf.io.decode_image(image_file, channels=channels)\n",
    "    # Resize the image\n",
    "    image_resized = tf.image.resize(image_tensor, [image_size, image_size])\n",
    "    # Normalize the image data\n",
    "    image_normalized = image_resized / 255.0\n",
    "    return image_normalized.numpy()\n",
    "\n",
    "def create_data_lists(filenames, labels):\n",
    "    \"\"\"Creates lists of images and labels.\"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for path, label in zip(filenames, labels):\n",
    "        image = load_and_preprocess_image(path)\n",
    "        X.append(image)\n",
    "        y.append(label)\n",
    "    \n",
    "    X_train = np.array(X)\n",
    "    y_train = np.array(y)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read csv files to get filepaths and labels \n",
    "df_train= pd.read_csv(\"data/FINAL_METER_ML_train_2000.csv\")\n",
    "df_val = pd.read_csv(\"data/FINAL_METER_ML_val.csv\")\n",
    "df_test = pd.read_csv(\"data/FINAL_METER_ML_test.csv\")\n",
    "\n",
    "# convert each string in the DataFrame to a list\n",
    "df_train['Label'] = df_train['Label'].apply(ast.literal_eval).apply(np.array)\n",
    "df_val['Label'] = df_val['Label'].apply(ast.literal_eval).apply(np.array)\n",
    "df_test['Label'] = df_test['Label'].apply(ast.literal_eval).apply(np.array)\n",
    "\n",
    "#define the test/train split in own arrays for images and labels\n",
    "X_train = df_train['Image_Folder']\n",
    "X_val = df_val['Image_Folder']\n",
    "X_test = df_test['Image_Folder']\n",
    "\n",
    "y_train = np.array(df_train['Label']).tolist()\n",
    "y_val = np.array(df_val['Label']).tolist()\n",
    "y_test = np.array(df_test['Label']).tolist()\n",
    "\n",
    "#creaet the image arrays by retrieving the image information from azure\n",
    "X_train, y_train = create_data_lists(X_train, y_train)\n",
    "X_val, y_val = create_data_lists(X_val, y_val)\n",
    "X_test, y_test = create_data_lists(X_test, y_test)\n",
    "\n",
    "# Save the array to a file to later use in models\n",
    "np.save('data/x_train.npy', X_train)\n",
    "np.save('data/x_test.npy', X_test)\n",
    "np.save('data/x_val.npy', X_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
