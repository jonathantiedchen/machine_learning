{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the follwing cell to pip install the necerssary packages specified in the requirements.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# to push/pull form ucloud enter following code in terminal\n",
    "git config --global user.name \"FIRST_NAME LAST_NAME\"\n",
    "git config --global user.email \"MY_NAME@example.com\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from azure.storage.blob import BlobServiceClient, ContainerClient\n",
    "from azure.core.exceptions import ResourceNotFoundError\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ast\n",
    "import time\n",
    "from keras.models import load_model\n",
    "import tempfile\n",
    "\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dense, Flatten, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from keras.utils import plot_model\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix\n",
    "from keras.callbacks import Callback, ReduceLROnPlateau, EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up storage\n",
    "connection_string = \"DefaultEndpointsProtocol=https;AccountName=mlfinalexam5505462853;AccountKey=0c40lghglG5/GlNK9yujDQAgo38GKoS2I3DeC/g22hwAEIFANKpmC/TqOpRk4RCT1DbfNiHBFt72+AStB+PfUA==;EndpointSuffix=core.windows.net\"\n",
    "container_name = \"meterml\"\n",
    "\n",
    "#create client\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "container_client = blob_service_client.get_container_client(container_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Image Paths and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get filepaths\n",
    "df = pd.read_csv(\"FINAL_METER_ML_train.csv\")\n",
    "df = df[:40000]\n",
    "\n",
    "# convert each string in the DataFrame to a list\n",
    "df['Label'] = df['Label'].apply(ast.literal_eval)\n",
    "\n",
    "# convert each list in the DataFrame to a numpy array\n",
    "df['Label'] = df['Label'].apply(np.array)\n",
    "\n",
    "class_names=[\"CAFOs\",\"Landfills\",\"Mines\",\"Negative\",\"ProcessingPlants\",\"RefineriesAndTerminals\",\"WWTreatment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions \n",
    "\n",
    "- Training Accuracy and Loss Graphs:    plot_history(history)\n",
    "- Predictions:                          print_predictions(inceptionv3_model, test_ds)\n",
    "- True and Predicted Classes:           true_classes,predicted_classes = true_pred_classes(inceptionv3_model, test_ds)\n",
    "- Accuracy:                             accuracy_score(true_classes,predicted_classes)\n",
    "- F1 Score:                             f1_score(true_classes, predicted_classes, average='weighted')\n",
    "- Recall:                               recall_score(true_classes, predicted_classes, average='weighted')\n",
    "- Precision:                            precision_score(true_classes, predicted_classes, average='weighted')\n",
    "- Confusion MAtrix (as Array):          conf_matrix = confusion_matrix(true_classes, predicted_classes)\n",
    "- Plot Confusion Matrix:                print_conf_matrix(true_classes, predicted_classes,class_names)\n",
    "- Upload a trained model to azure:      download_model_from_azure(\"model_name\")\n",
    "- Download a trained model from azure:  upload_model_to_azure(model, \"model_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size=224\n",
    "channels=3\n",
    "autotune = tf.data.experimental.AUTOTUNE # Adapt preprocessing and prefetching dynamically\n",
    "\n",
    "def data_split(df):\n",
    "    \"\"\"Splits and returns the dataset into training, validation, and test\"\"\"\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(df['Image_Folder'], df['Label'], test_size=0.15, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.176, random_state=42)\n",
    "    #convert labels to array\n",
    "    y_train = np.array(y_train).tolist()\n",
    "    y_val = np.array(y_val).tolist()\n",
    "    y_test = np.array(y_test).tolist()\n",
    "    #print number of observations per datasets\n",
    "    print(\"Nr. Training:\",len(X_train),\"Nr. Validation:\",len(X_val),\"Nr. Test:\",len(X_test))\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "def load_image(path):\n",
    "    \"\"\"Load an image from Azure Blob Storage.\"\"\"\n",
    "    blob_client = container_client.get_blob_client(path)\n",
    "    blob_data = blob_client.download_blob().readall()  # Directly read all bytes\n",
    "    \n",
    "    return io.BytesIO(blob_data)\n",
    "\n",
    "\n",
    "def load_and_preprocess_image(path):\n",
    "    \"\"\"Loads an image, decodes it to grayscale, resizes, and normalizes it.\"\"\"\n",
    "    # Load image\n",
    "    image_file = load_image(path.numpy().decode('utf-8'))\n",
    "    # Decode the image to grayscale\n",
    "    image_tensor = tf.io.decode_image(image_file.getvalue(), channels=channels)\n",
    "    # Resize the image\n",
    "    image_resized = tf.image.resize(image_tensor, [image_size, image_size])\n",
    "    # Normalize the image data\n",
    "    image_normalized = image_resized / 255.0\n",
    "    return image_normalized\n",
    "\n",
    "\n",
    "def process_tensor(path, label):\n",
    "    \"\"\"Function to load an image from blob storage, decode, resize, and normalize it.\"\"\"\n",
    "    image_normalized = tf.py_function(load_and_preprocess_image, [path], tf.float32)\n",
    "    # Ensure the shape is set correctly for grayscale\n",
    "    image_normalized.set_shape([image_size, image_size, channels])\n",
    "    return image_normalized, label\n",
    "\n",
    "\n",
    "def create_dataset(filenames, labels, is_training=True):\n",
    "    \"\"\"Creates a TensorFlow dataset from filenames and labels.\"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "    dataset = dataset.map(process_tensor, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    #shuffle the data when it is the training dataset\n",
    "    if is_training:\n",
    "        dataset = dataset.cache()\n",
    "        dataset = dataset.shuffle(buffer_size=1024)\n",
    "    #creates batches    \n",
    "    dataset = dataset.batch(256)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def create_all_datasets(X_train, X_val, X_test, y_train, y_val, y_test ):\n",
    "    \"\"\"Creates train, test, and val datasets by calling the create_dataset function each.\"\"\"\n",
    "    train_ds = create_dataset(X_train, y_train)\n",
    "    test_ds = create_dataset(X_test, y_test, False)\n",
    "    val_ds = create_dataset(X_val, y_val, False)\n",
    "    \n",
    "    return train_ds, test_ds, val_ds\n",
    "\n",
    "\n",
    "def print_dataset(dataset):\n",
    "    \"\"\"Print the plain dataset.\"\"\"\n",
    "    for images, labels in dataset.take(1):  # Here, take(1) takes the first batch\n",
    "        print(\"Images:\", images.numpy())  # Convert tensor to numpy array and print\n",
    "        print(\"Labels:\", labels.numpy())  # Convert tensor to numpy array and print\n",
    "\n",
    "\n",
    "def plot_history(model):\n",
    "    \"\"\"Plots the accuracy and loss of the inputted model.\"\"\"\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(model.history['accuracy'])\n",
    "    plt.plot(model.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    # summarize history for loss\n",
    "    plt.plot(model.history['loss'])\n",
    "    plt.plot(model.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def print_predictions(model, ds):\n",
    "    \"\"\"Predictions based on test dataset.\"\"\"\n",
    "    #predict\n",
    "    for images, labels in ds:\n",
    "        predictions = model.predict(images)  # Only pass image data\n",
    "        classes = predictions.argmax(axis=-1) #selects biggest value as prediction\n",
    "\n",
    "        for pred, classe, label in zip(predictions,classes, labels):\n",
    "            print(\"Prediction:\", pred,\"Pred. Class: \",classe, \"Actual Label:\", label.numpy())# Print the first prediction\n",
    "        break\n",
    "    \n",
    "        \n",
    "def plot_model(model): \n",
    "    \"\"\"Plot model with predefined arguments.\"\"\"\n",
    "    plot_model(model, \n",
    "            to_file='vgg.png',\n",
    "            show_shapes=True,\n",
    "            show_dtype=True,\n",
    "            show_layer_names=True,\n",
    "            show_layer_activations=True,\n",
    "            show_trainable=False)\n",
    "    \n",
    "\n",
    "def evaluate_model(model, test_ds):\n",
    "    result = model.evaluate(test_ds)\n",
    "    # Assuming accuracy was the second metric (index 1), extract the accuracy.\n",
    "    test_accuracy = result[1] * 100  # Convert to percentage\n",
    "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    return test_accuracy\n",
    "\n",
    "\n",
    "def true_pred_classes(model, dataset): \n",
    "    \"\"\"\n",
    "    Evaluates the given model using the dataset.\n",
    "    Returns: accuracy, f1, recall, precision, confusion matrix\n",
    "    \"\"\"\n",
    "    # Collect all labels and predictions\n",
    "    true_classes = []\n",
    "    predicted_classes = []\n",
    "\n",
    "    # Iterate over the dataset\n",
    "    for images, labels in dataset:\n",
    "        # Predict batch\n",
    "        preds = model.predict(images)\n",
    "        preds = np.argmax(preds, axis=1)\n",
    "        true = labels.numpy()  # Assuming labels are already integer-encoded\n",
    "\n",
    "        # Append to lists\n",
    "        true_classes.extend(true)\n",
    "        predicted_classes.extend(preds)\n",
    "    return true_classes,predicted_classes\n",
    "\n",
    "\n",
    "def print_conf_matrix(true_classes, predicted_classes, class_names):\n",
    "    \"\"\"\n",
    "    Print confusion matrix.\n",
    "    \"\"\"\n",
    "    conf_matrix = confusion_matrix(true_classes, predicted_classes)\n",
    "    df_cm = pd.DataFrame(\n",
    "        conf_matrix, index=class_names, columns=class_names,\n",
    "    )\n",
    "    fig = plt.figure(figsize=(10,7))\n",
    "    heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\", cmap='Blues')\n",
    "\n",
    "    # Set aesthetics for better readability\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=14)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=14)\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def upload_model_to_azure(model, model_name):\n",
    "    # Save the model locally\n",
    "    local_model_path = f\"{model_name}.keras\"\n",
    "    model.save(local_model_path)\n",
    "    \n",
    "    # Get the blob client\n",
    "    blob_client = blob_service_client.get_blob_client(container='meterml', blob=f'models/{model_name}.keras')\n",
    "    \n",
    "    # Upload the model file to Azure Blob Storage\n",
    "    with open(local_model_path, 'rb') as data:\n",
    "        blob_client.upload_blob(data, overwrite=True)\n",
    "    \n",
    "    # Optionally, delete the local model file after upload\n",
    "    os.remove(local_model_path)\n",
    "    \n",
    "    print(\"Model successfully stored in Azure.\")\n",
    "\n",
    "\n",
    "def download_model_from_azure(model_name):\n",
    "    # Construct the blob path\n",
    "    blob_path = f'models/{model_name}.keras'\n",
    "    blob_client = blob_service_client.get_blob_client(container='meterml', blob=blob_path)\n",
    "    \n",
    "    try:\n",
    "        # Download the blob to a temporary file\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix='.keras') as temp_file:\n",
    "            temp_file.write(blob_client.download_blob().readall())\n",
    "            temp_file_path = temp_file.name\n",
    "\n",
    "        # Load the model using TensorFlow Keras\n",
    "        model = tf.keras.models.load_model(temp_file_path)\n",
    "        \n",
    "    except ResourceNotFoundError:\n",
    "        print(f\"Error: The model '{model_name}' was not found in Azure Blob Storage.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the model: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        if 'temp_file_path' in locals():\n",
    "            # Optionally, delete the temporary file\n",
    "            os.remove(temp_file_path)\n",
    "\n",
    "    print(\"Model loaded. Enter model.summary() to print a model summary.\")\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = data_split(df)\n",
    "    \n",
    "train_ds, test_ds, val_ds = create_all_datasets(X_train, X_val, X_test, y_train, y_val, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom, RandomContrast\n",
    "import random\n",
    "\n",
    "\n",
    "augmentation_model = tf.keras.Sequential([\n",
    "    RandomFlip(\"horizontal_and_vertical\"),\n",
    "    RandomRotation(0.2),\n",
    "    RandomZoom(0.2),\n",
    "    RandomContrast(0.2),\n",
    "])\n",
    "\n",
    "# Function to augment images\n",
    "def augment_image(image):\n",
    "    image = tf.expand_dims(image, 0)  # Add batch dimension\n",
    "    image = augmentation_model(image)\n",
    "    image = tf.squeeze(image, 0)  # Remove batch dimension\n",
    "    return image\n",
    "\n",
    "\n",
    "# Extract labels and convert them to a list of tuples\n",
    "labels = []\n",
    "for _, label in train_ds:\n",
    "    labels.extend(label.numpy())\n",
    "\n",
    "# Convert each label to a tuple (since numpy arrays are not hashable)\n",
    "labels_tuples = [tuple(label) for label in labels]\n",
    "\n",
    "# Count the unique labels\n",
    "label_counts = Counter(labels_tuples)\n",
    "\n",
    "# Print the unique labels and their counts\n",
    "#for label, count in label_counts.items():\n",
    "#    print(f\"Label: {label}, Count: {count}\")\n",
    "    \n",
    "# Calculate the difference between all other labels and the majority label\n",
    "majority_label = max(label_counts, key=label_counts.get)\n",
    "majority_count = label_counts[majority_label]\n",
    "\n",
    "label_differences = {}\n",
    "for label, count in label_counts.items():\n",
    "    if label != majority_label:\n",
    "        difference = majority_count - count\n",
    "        label_differences[label] = difference\n",
    "        print(f\"Label: {label}, Difference: {difference}\")\n",
    "        \n",
    "# Calculate the number of images to augment for each label\n",
    "max_count = max(label_counts.values())\n",
    "augmentation_needed = {label: max_count - count for label, count in label_counts.items() if count < max_count}\n",
    "type(augmentation_needed)\n",
    "\n",
    "# Augment images and add to the dataset\n",
    "augmented_images = []\n",
    "augmented_labels = []\n",
    "\n",
    "\n",
    "\n",
    "for label, difference in label_differences.items():\n",
    "    target_images = []\n",
    "    for image, y_label in zip(X_train, y_train):\n",
    "        if tuple(y_label) == label:\n",
    "            target_images.append(image)\n",
    "\n",
    "    for _ in range(difference):\n",
    "        r_image = random.choice(target_images)\n",
    "        augmented_images.append(r_image)\n",
    "        augmented_labels.append(label)\n",
    "\n",
    "augmented_images = pd.Series(augmented_images, name=\"Image_Folder\")\n",
    "augmented_labels = [np.array(lst) for lst in augmented_labels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom, RandomContrast, RandomBrightness\n",
    "\n",
    "# Define constants\n",
    "image_size = 224  # Example image size\n",
    "channels = 3  # Assuming grayscale\n",
    "\n",
    "# Create the augmentation model outside of the function\n",
    "augmentation_model = tf.keras.Sequential([\n",
    "    RandomBrightness(0.4),\n",
    "    RandomFlip(\"horizontal_and_vertical\"),\n",
    "    RandomRotation(0.4),\n",
    "    RandomZoom(0.4),\n",
    "    RandomContrast(0.5)\n",
    "])\n",
    "\n",
    "def augment_image(image):\n",
    "    image = tf.expand_dims(image, 0)  # Add batch dimension\n",
    "    image = augmentation_model(image, training=True)\n",
    "    image = tf.squeeze(image, 0)  # Remove batch dimension\n",
    "    return image\n",
    "\n",
    "# Assuming container_client is already set up for Azure Blob Storage\n",
    "def load_image(path):\n",
    "    \"\"\"Load an image from Azure Blob Storage.\"\"\"\n",
    "    blob_client = container_client.get_blob_client(path)\n",
    "    blob_data = blob_client.download_blob().readall()  # Directly read all bytes\n",
    "    return io.BytesIO(blob_data)\n",
    "\n",
    "def _load_and_process_image(path):\n",
    "        # Load image\n",
    "        image_file = load_image(path.numpy().decode('utf-8'))\n",
    "        # Decode the image to grayscale\n",
    "        image_tensor = tf.io.decode_image(image_file.getvalue(), channels=channels)\n",
    "        # Resize the image\n",
    "        image_resized = tf.image.resize(image_tensor, [image_size, image_size])\n",
    "        # Normalize the image data\n",
    "        augmented_image = augment_image(image_resized)\n",
    "        image_normalized = augmented_image / 255.0\n",
    "        return image_normalized\n",
    "\n",
    "def aug_process_tensor(path, label):\n",
    "\n",
    "    image_normalized = tf.py_function(func=_load_and_process_image, inp=[path], Tout=tf.float32)\n",
    "    image_normalized.set_shape([image_size, image_size, channels])\n",
    "    \n",
    "    #augmented_image = augment_image(image_normalized)\n",
    "    augmented_image = image_normalized\n",
    "    augmented_image.set_shape([image_size, image_size, channels])\n",
    "    \n",
    "    return augmented_image, label\n",
    "\n",
    "def create_aug_dataset(filenames, labels, is_training=True):\n",
    "    \"\"\"Creates a TensorFlow dataset from filenames and labels.\"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "    dataset = dataset.map(aug_process_tensor, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if is_training:\n",
    "        dataset = dataset.cache()\n",
    "        dataset = dataset.shuffle(buffer_size=1024)\n",
    "    dataset = dataset.batch(256)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "# Assuming augmented_images and augmented_labels are defined elsewhere\n",
    "aug_dataset = create_aug_dataset(augmented_images, augmented_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the combined dataset\n",
    "test_ds = test_ds.concatenate(aug_dataset)\n",
    "\n",
    "\n",
    "buffer_size = len(test_ds) + len(aug_dataset)  # Set buffer size to the size of the combined dataset\n",
    "test_ds = test_ds.shuffle(buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_first_image_from_dataset(dataset, index):\n",
    "    # Take one batch from the dataset\n",
    "    for images, labels in dataset.take(index):\n",
    "        # Assuming the image tensor is in the shape [batch_size, height, width, channels]\n",
    "        # and you need the first image in the batch\n",
    "        first_image = images[0]  # This is a tensor\n",
    "\n",
    "        # Check if the image needs to be squeezed (in case it's a grayscale image with a single channel)\n",
    "        if first_image.shape[-1] == 1:\n",
    "            first_image = tf.squeeze(first_image, axis=-1)\n",
    "        \n",
    "        # Convert tensor to numpy for plotting\n",
    "        first_image_np = first_image.numpy()\n",
    "\n",
    "        # Plot the image\n",
    "        plt.imshow(first_image_np, cmap='gray')\n",
    "        plt.title(f'Label: {labels[0].numpy()}')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Example usage with your train_ds dataset\n",
    "plot_first_image_from_dataset(test_ds,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet50: \n",
    "- https://datagen.tech/guides/computer-vision/resnet-50/\n",
    "- https://medium.com/@bravinwasike18/building-a-deep-learning-model-with-keras-and-resnet-50-9dd6f4eb3351\n",
    "- https://medium.com/@ozgunhaznedar/image-classification-on-satellite-images-with-deep-learning-baa9813dde4e\n",
    "- https://wandb.ai/mostafaibrahim17/ml-articles/reports/The-Basics-of-ResNet50---Vmlldzo2NDkwNDE2#step-4:-building-resnet-50-model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set variables\n",
    "train_epochs = 30\n",
    "tune_epochs = 10\n",
    "total_epochs = train_epochs + tune_epochs\n",
    "batch_size = 128\n",
    "\n",
    "# import resnet model for transfer learning\n",
    "rn50_base = tf.keras.applications.ResNet50(\n",
    "    include_top = False,\n",
    "    weights = \"imagenet\",\n",
    "    input_shape=(224,224,3)\n",
    "    )\n",
    "\n",
    "# Freeze layers pf basemodel, so the pre-trained weights are fixed\n",
    "for each_layer in rn50_base.layers:\n",
    "        each_layer.trainable=False\n",
    "\n",
    "# create sequential model\n",
    "resnet_model = Sequential()\n",
    "\n",
    "# Add output layers for finetuning\n",
    "resnet_model.add(rn50_base)\n",
    "resnet_model.add(Flatten()) #use flatten instead of GlobalAveragePooling2D as it may yield better results when enough data\n",
    "resnet_model.add(Dense(512, activation='relu'))\n",
    "resnet_model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "resnet_model.compile(optimizer= tf.keras.optimizers.Adam(),\n",
    "                    loss= tf.keras.losses.CategoricalCrossentropy(), \n",
    "                    metrics = ['accuracy'])\n",
    "\n",
    "#initializt time\n",
    "time.time()\n",
    "\n",
    "# Train model \n",
    "history = resnet_model.fit(train_ds, \n",
    "                        validation_data = val_ds, \n",
    "                        epochs = epochs, \n",
    "                        batch_size=batch_size, \n",
    "                        callbacks = [ReduceLROnPlateau(patience=5), EarlyStopping(patience=10)])\n",
    "\n",
    "#print time in seconds\n",
    "print(\"Training time in seconds:\", time.time()-t0)\n",
    "\n",
    "#save model\n",
    "resnet_model.save(\"resnet_model.keras\")\n",
    "\n",
    "# Plot model information\n",
    "plot_history(history)\n",
    "\n",
    "#save model to azure\n",
    "upload_model_to_azure(resnet_model, \"resnet_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unfreeze last convolution layer for fine tuning\n",
    "for each_layer in rn50_base.layers:\n",
    "        each_layer.trainable=False\n",
    "for layer in [l for l in rn50_base.layers if 'conv5' in l.name]:\n",
    "   layer.trainable = True\n",
    "   \n",
    "for i, layer in enumerate(rn50_base.layers):\n",
    "    print(i, layer.name, \"-\", layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the model with smaller learning rate\n",
    "resnet_model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0001),\n",
    "                    loss = tf.keras.losses.CategoricalCrossentropy(), \n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "#initialize timing\n",
    "t0 = time.time()\n",
    "\n",
    "# Train model \n",
    "history = resnet_model.fit(train_ds, \n",
    "                        validation_data = val_ds, \n",
    "                        epochs=total_epochs, \n",
    "                        batch_size=batch_size, \n",
    "                        callbacks = [ReduceLROnPlateau(patience=5), EarlyStopping(patience=10)])\n",
    "\n",
    "print(\"Training time in seconds:\", time.time()-t0)\n",
    "\n",
    "#save trained model\n",
    "upload_model_to_azure(resnet_model,\"resnet_tf_model\")\n",
    "\n",
    "# Plot training plots\n",
    "plot_history(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
