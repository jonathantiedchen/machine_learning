{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: azure-storage-blob in /opt/anaconda3/lib/python3.8/site-packages (12.19.1)\n",
      "Requirement already satisfied: azure-core<2.0.0,>=1.28.0 in /opt/anaconda3/lib/python3.8/site-packages (from azure-storage-blob) (1.30.1)\n",
      "Requirement already satisfied: cryptography>=2.1.4 in /opt/anaconda3/lib/python3.8/site-packages (from azure-storage-blob) (3.4.7)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/anaconda3/lib/python3.8/site-packages (from azure-storage-blob) (4.11.0)\n",
      "Requirement already satisfied: isodate>=0.6.1 in /opt/anaconda3/lib/python3.8/site-packages (from azure-storage-blob) (0.6.1)\n",
      "Requirement already satisfied: requests>=2.21.0 in /opt/anaconda3/lib/python3.8/site-packages (from azure-core<2.0.0,>=1.28.0->azure-storage-blob) (2.25.1)\n",
      "Requirement already satisfied: six>=1.11.0 in /opt/anaconda3/lib/python3.8/site-packages (from azure-core<2.0.0,>=1.28.0->azure-storage-blob) (1.15.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/anaconda3/lib/python3.8/site-packages (from cryptography>=2.1.4->azure-storage-blob) (1.14.5)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/lib/python3.8/site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob) (2.20)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/anaconda3/lib/python3.8/site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/lib/python3.8/site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.8/site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (2023.7.22)\n",
      "\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.8/site-packages (1.6.2)\n",
      "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /opt/anaconda3/lib/python3.8/site-packages (from scipy) (1.22.2)\n",
      "\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.8/site-packages (0.24.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn) (1.22.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn) (1.6.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn) (2.1.0)\n",
      "\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: opencv-python in /opt/anaconda3/lib/python3.8/site-packages (4.9.0.80)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /opt/anaconda3/lib/python3.8/site-packages (from opencv-python) (1.22.2)\n",
      "\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install azure-storage-blob\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "import json\n",
    "import os\n",
    "from PIL import Image, ImageStat\n",
    "import matplotlib.pyplot as plt\n",
    "import io  # Used to convert bytes to a file-like object\n",
    "import pandas as pd\n",
    "import re\n",
    "%pip install scipy\n",
    "%pip install scikit-learn\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "%pip install opencv-python\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up storage for reading meta data\n",
    "connection_string = \"DefaultEndpointsProtocol=https;AccountName=mlfinalexam5505462853;AccountKey=0c40lghglG5/GlNK9yujDQAgo38GKoS2I3DeC/g22hwAEIFANKpmC/TqOpRk4RCT1DbfNiHBFt72+AStB+PfUA==;EndpointSuffix=core.windows.net\"\n",
    "container_name = \"publicdata\"\n",
    "\n",
    "#create client\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "container_client = blob_service_client.get_container_client(container_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all blobs and process JSON files\n",
    "json_data = {}\n",
    "\n",
    "blobs = container_client.list_blobs()\n",
    "for blob in blobs:\n",
    "    if blob.name.endswith(\".json\"):\n",
    "        # Get the blob content\n",
    "        blob_client = container_client.get_blob_client(blob.name)\n",
    "        blob_content = blob_client.download_blob().content_as_text()\n",
    "\n",
    "        # Convert to JSON and add to the dictionary using the blob's name as the key\n",
    "        json_data[blob.name] = json.loads(blob_content)  # Now storing JSON content as a dictionary\n",
    "\n",
    "# #Assigning specific JSON data to variables\n",
    "# category = json_data.get(\"v1.0-mini/category.json\", [])\n",
    "# sensor = json_data.get(\"v1.0-mini/sensor.json\", {})\n",
    "# surface_ann = json_data.get(\"v1.0-mini/surface_ann.json\", {})\n",
    "# attribute = json_data.get(\"v1.0-mini/attribute.json\", {})\n",
    "# log = json_data.get(\"v1.0-mini/log.json\", {})\n",
    "# calibrated_sensor = json_data.get(\"v1.0-mini/calibrated_sensor.json\", {})\n",
    "# sample_data = json_data.get(\"v1.0-mini/sample_data.json\", {})\n",
    "# sample = json_data.get(\"v1.0-mini/sample.json\", {})\n",
    "# ego_pose = json_data.get(\"v1.0-mini/ego_pose.json\", {})\n",
    "# object_ann = json_data.get(\"v1.0-mini/object_ann.json\", {})\n",
    "\n",
    "default_path_train = \"all-metadata/v1.0-train/\"\n",
    "default_path_test = \"all-metadata/v1.0-test/\"\n",
    "default_path_val = \"all-metadata/v1.0-val/\"\n",
    "\n",
    "\n",
    "def load_jsons(default_path):\n",
    "    object_ann = json_data.get(default_path + \"object_ann.json\", {})\n",
    "    sample_data = json_data.get(default_path + \"sample_data.json\", {})\n",
    "    category = json_data.get(default_path + \"category.json\", [])\n",
    "\n",
    "    return object_ann, sample_data, category\n",
    "\n",
    "object_ann_train, sample_data_train, category_train = load_jsons(default_path_train)\n",
    "#object_ann_test, sample_data_test, category_test = load_jsons(default_path_test)\n",
    "#object_ann_val, sample_data_val, category_val = load_jsons(default_path_val)\n",
    "#object_ann_test, sample_data_test, category_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Category Clustering and Label Encodeing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to dataframe for better processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_df(object_ann, sample_data, category):\n",
    "    #turn into proper json\n",
    "    object_ann_json = json.dumps(object_ann, indent=4)\n",
    "    sample_data_json = json.dumps(sample_data, indent=4)\n",
    "    category_json = json.dumps(category, indent=4)\n",
    "    #turn into dataframe\n",
    "    object_ann_df = pd.read_json(object_ann_json)\n",
    "    category_df = pd.read_json(category_json)\n",
    "    sample_data_df = pd.read_json(sample_data_json)\n",
    "    return object_ann_df, sample_data_df, category_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_ann_df_train, sample_data_df_train, category_df_train = json_to_df(object_ann_train, sample_data_train, category_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_ann_df_train, sample_data_df_train, category_df_train = json_to_df(object_ann_train, sample_data_train, category_train)\n",
    "#object_ann_df_test, sample_data_df_test, category_df_test = json_to_df(object_ann_test, sample_data_test, category_test)\n",
    "#object_ann_df_val, sample_data_df_val, category_df_val = json_to_df(object_ann_val, sample_data_val, category_val)\n",
    "#object_ann_df_test.shape, sample_data_df_test.shape, category_df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster chosen Categories into right \"Parent\" - category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clustering of categories\n",
    "def transform_category(category_str):\n",
    "    if re.match(r'^human\\.', category_str):\n",
    "        return 'Human'\n",
    "    if re.match(r'^movable_object\\.barrier', category_str):\n",
    "        return 'Barrier'\n",
    "    if re.match(r'^movable_object\\.cone', category_str):\n",
    "        return 'Cone'\n",
    "    if re.match(r'^vehicle\\.bicycle', category_str):\n",
    "        return 'Bike'\n",
    "    if re.match(r'^vehicle\\.motorcycle', category_str):\n",
    "        return 'Motorcycle'\n",
    "    if re.match(r'^vehicle\\.truck', category_str):\n",
    "        return 'Truck'\n",
    "    if re.match(r'^vehicle\\.car', category_str):\n",
    "        return 'Car'\n",
    "    if re.match(r'^movable_object\\.trafficcone', category_str):\n",
    "        return 'Trafficcone'\n",
    "    return None\n",
    "\n",
    "def category_proc(category_df):\n",
    "    #apply function to the category column -> clustering of chosen categories\n",
    "    category_df['name'] = category_df[\"name\"].apply(transform_category)\n",
    "\n",
    "    #remove description\n",
    "    category_df = category_df.drop(columns=\"description\")\n",
    "\n",
    "    #drop rows that are not included in the chosen categories\n",
    "    category_df = category_df.dropna(subset=['name'])\n",
    "    return category_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63a94dfa99bb47529567cd90d3b58384</td>\n",
       "      <td>None</td>\n",
       "      <td>All animals, e.g. cats, rats, dogs, deer, birds.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a86329ee68a0411fb426dcad3b21452f</td>\n",
       "      <td>None</td>\n",
       "      <td>Surfaces should be regarded with no concern of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1fa93b757fc74fb197cdd60001ad8abf</td>\n",
       "      <td>Human</td>\n",
       "      <td>Adult subcategory.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b1c6de4c57f14a5383d9f963fbdcb5cb</td>\n",
       "      <td>Human</td>\n",
       "      <td>Child subcategory.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>909f1237d34a49d6bdd27c2fe4581d79</td>\n",
       "      <td>Human</td>\n",
       "      <td>Construction worker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>403fede16c88426885dd73366f16c34a</td>\n",
       "      <td>Human</td>\n",
       "      <td>A small electric or self-propelled vehicle, e....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>e3c7da112cd9475a9a10d45015424815</td>\n",
       "      <td>Human</td>\n",
       "      <td>Police officer.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6a5888777ca14867a8aee3fe539b56c4</td>\n",
       "      <td>Human</td>\n",
       "      <td>Strollers. If a person is in the stroller, inc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>b2d7c6c701254928a9e4d6aac9446d79</td>\n",
       "      <td>Human</td>\n",
       "      <td>Wheelchairs. If a person is in the wheelchair,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>653f7efbb9514ce7b81d44070d6208c1</td>\n",
       "      <td>Barrier</td>\n",
       "      <td>Temporary road barrier placed in the scene in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>063c5e7f638343d3a7230bc3641caf97</td>\n",
       "      <td>None</td>\n",
       "      <td>Movable object that is left on the driveable s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>d772e4bae20f493f98e15a76518b31d7</td>\n",
       "      <td>None</td>\n",
       "      <td>Objects that a pedestrian may push or pull. Fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>85abebdccd4d46c7be428af5a6173947</td>\n",
       "      <td>Trafficcone</td>\n",
       "      <td>All types of traffic cone.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0a30519ee16a4619b4f4acfe2d78fb55</td>\n",
       "      <td>None</td>\n",
       "      <td>Area or device intended to park or secure the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>fc95c87b806f48f8a1faea2dcc2222a4</td>\n",
       "      <td>Bike</td>\n",
       "      <td>Human or electric powered 2-wheeled vehicle de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>003edbfb9ca849ee8a7496e9af3025d4</td>\n",
       "      <td>None</td>\n",
       "      <td>Bendy bus subcategory. Annotate each section o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>fedb11688db84088883945752e480c2c</td>\n",
       "      <td>None</td>\n",
       "      <td>Rigid bus subcategory.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>fd69059b62a3469fbaef25340c0eab7f</td>\n",
       "      <td>Car</td>\n",
       "      <td>Vehicle designed primarily for personal use, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5b3cd6f2bca64b83aa3d0008df87d0e4</td>\n",
       "      <td>None</td>\n",
       "      <td>Vehicles primarily designed for construction. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7754874e6d0247f9855ae19a4028bf0e</td>\n",
       "      <td>None</td>\n",
       "      <td>Ego vehicle.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>732cce86872640628788ff1bb81006d4</td>\n",
       "      <td>None</td>\n",
       "      <td>All types of ambulances.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>7b2ff083a64e4d53809ae5d9be563504</td>\n",
       "      <td>None</td>\n",
       "      <td>All types of police vehicles including police ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>dfd26f200ade4d24b540184e16050022</td>\n",
       "      <td>Motorcycle</td>\n",
       "      <td>Gasoline or electric powered 2-wheeled vehicle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>90d0f6f8e7c749149b1b6c3a029841a8</td>\n",
       "      <td>None</td>\n",
       "      <td>Any vehicle trailer, both for trucks, cars and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>6021b5187b924d64be64a702e5570edf</td>\n",
       "      <td>Truck</td>\n",
       "      <td>Vehicles primarily designed to haul cargo incl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               token         name  \\\n",
       "0   63a94dfa99bb47529567cd90d3b58384         None   \n",
       "1   a86329ee68a0411fb426dcad3b21452f         None   \n",
       "2   1fa93b757fc74fb197cdd60001ad8abf        Human   \n",
       "3   b1c6de4c57f14a5383d9f963fbdcb5cb        Human   \n",
       "4   909f1237d34a49d6bdd27c2fe4581d79        Human   \n",
       "5   403fede16c88426885dd73366f16c34a        Human   \n",
       "6   e3c7da112cd9475a9a10d45015424815        Human   \n",
       "7   6a5888777ca14867a8aee3fe539b56c4        Human   \n",
       "8   b2d7c6c701254928a9e4d6aac9446d79        Human   \n",
       "9   653f7efbb9514ce7b81d44070d6208c1      Barrier   \n",
       "10  063c5e7f638343d3a7230bc3641caf97         None   \n",
       "11  d772e4bae20f493f98e15a76518b31d7         None   \n",
       "12  85abebdccd4d46c7be428af5a6173947  Trafficcone   \n",
       "13  0a30519ee16a4619b4f4acfe2d78fb55         None   \n",
       "14  fc95c87b806f48f8a1faea2dcc2222a4         Bike   \n",
       "15  003edbfb9ca849ee8a7496e9af3025d4         None   \n",
       "16  fedb11688db84088883945752e480c2c         None   \n",
       "17  fd69059b62a3469fbaef25340c0eab7f          Car   \n",
       "18  5b3cd6f2bca64b83aa3d0008df87d0e4         None   \n",
       "19  7754874e6d0247f9855ae19a4028bf0e         None   \n",
       "20  732cce86872640628788ff1bb81006d4         None   \n",
       "21  7b2ff083a64e4d53809ae5d9be563504         None   \n",
       "22  dfd26f200ade4d24b540184e16050022   Motorcycle   \n",
       "23  90d0f6f8e7c749149b1b6c3a029841a8         None   \n",
       "24  6021b5187b924d64be64a702e5570edf        Truck   \n",
       "\n",
       "                                          description  \n",
       "0    All animals, e.g. cats, rats, dogs, deer, birds.  \n",
       "1   Surfaces should be regarded with no concern of...  \n",
       "2                                  Adult subcategory.  \n",
       "3                                  Child subcategory.  \n",
       "4                                 Construction worker  \n",
       "5   A small electric or self-propelled vehicle, e....  \n",
       "6                                     Police officer.  \n",
       "7   Strollers. If a person is in the stroller, inc...  \n",
       "8   Wheelchairs. If a person is in the wheelchair,...  \n",
       "9   Temporary road barrier placed in the scene in ...  \n",
       "10  Movable object that is left on the driveable s...  \n",
       "11  Objects that a pedestrian may push or pull. Fo...  \n",
       "12                         All types of traffic cone.  \n",
       "13  Area or device intended to park or secure the ...  \n",
       "14  Human or electric powered 2-wheeled vehicle de...  \n",
       "15  Bendy bus subcategory. Annotate each section o...  \n",
       "16                             Rigid bus subcategory.  \n",
       "17  Vehicle designed primarily for personal use, e...  \n",
       "18  Vehicles primarily designed for construction. ...  \n",
       "19                                       Ego vehicle.  \n",
       "20                           All types of ambulances.  \n",
       "21  All types of police vehicles including police ...  \n",
       "22  Gasoline or electric powered 2-wheeled vehicle...  \n",
       "23  Any vehicle trailer, both for trucks, cars and...  \n",
       "24  Vehicles primarily designed to haul cargo incl...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_df_train_new = category_proc(category_df_train)\n",
    "#category_df_test = category_proc(category_df_test)\n",
    "#category_df_val = category_proc(category_df_val)\n",
    "category_df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Dataframes based on Data model and FK-PK dependencies & proper encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def df_merging_encoding(object_ann_df, sample_data_df, category_df):\n",
    "\n",
    "    label_data_v2 = None\n",
    "\n",
    "    #merge dataframes based on foreign keys to connect labeling with image data\n",
    "    obj_cat = pd.merge(object_ann_df, category_df, left_on='category_token', right_on='token', how='inner')\n",
    "    obj_cat = obj_cat.dropna(subset=['name'])\n",
    "\n",
    "    #remove annotations, where bboxes are too small -> after resizing not properly visible anymore\n",
    "    \n",
    "\n",
    "    #remove list of columns from dataframe\n",
    "    columns_to_remove = [\"token_x\", \"token_y\", \"category_token\", \"bbox\", \"mask\", \"attribute_tokens\", ]\n",
    "    obj_cat = obj_cat.drop(columns=columns_to_remove)\n",
    "\n",
    "    #filter so that only key frames are included\n",
    "    sample_data_df = sample_data_df[sample_data_df[\"is_key_frame\"] == True]\n",
    "\n",
    "    #merge with sample data\n",
    "    label_data_v1 = pd.merge(sample_data_df, obj_cat, left_on='token', right_on='sample_data_token', how='left')\n",
    "    label_data_v1['name'] = label_data_v1['name'].fillna('empty')\n",
    "    #group to see all labels for each image\n",
    "    label_data_v1 = label_data_v1.groupby(\"filename\")[\"name\"].value_counts().rename(\"count\").reset_index()\n",
    "    \n",
    "    \n",
    "    label_data_v1.to_excel(\"labeled.xlsx\")\n",
    "\n",
    "    #getting files with more than 5 annotations\n",
    "    \n",
    "    # files_to_drop = label_data_v1.groupby(\"filename\")[\"count\"].sum().reset_index()\n",
    "    # files_to_drop = files_to_drop[files_to_drop[\"count\"] > 5]\n",
    "\n",
    "    #turn labels from names to label_data_v1 variables\n",
    "    dummies = pd.get_dummies(label_data_v1['name'])\n",
    "\n",
    "\n",
    "\n",
    "    # Join the dummy variables to the main dataframe\n",
    "    labeled = pd.concat([label_data_v1, dummies], axis=1)\n",
    "    labeled[dummies.columns] = labeled[dummies.columns].astype(int)\n",
    "\n",
    "    # Drop the original 'name' column\n",
    "    labeled = labeled.drop(['name'], axis=1)\n",
    "    labeled = labeled.drop(['empty'], axis=1)\n",
    "    labeled = labeled.drop(['count'], axis=1)\n",
    "\n",
    "    # Group by 'filename' and aggregate the data\n",
    "    labeled = labeled.groupby('filename').agg({\n",
    "        'Human': 'max',\n",
    "        'Barrier': 'max',\n",
    "        'Bike': 'max',\n",
    "        'Motorcycle': 'max',\n",
    "        'Truck': 'max',\n",
    "        'Car': 'max',\n",
    "        'Trafficcone': 'max'\n",
    "    }).reset_index()\n",
    "\n",
    "    #removing the images with more than 5 annotations\n",
    "    \n",
    "    # labeled = labeled[~labeled['filename'].isin(files_to_drop['filename'])]\n",
    "    # print(\"Shape: \", labeled.shape)\n",
    "    \n",
    "    #labeled = labeled.drop(columns=\"Total\")\n",
    "    labeled.to_excel(\"labeled.xlsx\")\n",
    "\n",
    "    labeled['Total'] = labeled[[\"Human\", \"Barrier\", \"Bike\", \"Motorcycle\", \"Truck\", \"Car\", \"Trafficcone\"]].sum(axis=1)\n",
    "    #labeled['NoDetec'] = labeled[[\"Human\", \"Barrier\", \"Bike\", \"Motorcycle\", \"Truck\", \"Car\", \"Trafficcone\"]].apply(lambda row: 1 if (row == 0).all() else 0, axis=1)\n",
    "    return labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>Human</th>\n",
       "      <th>Barrier</th>\n",
       "      <th>Bike</th>\n",
       "      <th>Motorcycle</th>\n",
       "      <th>Truck</th>\n",
       "      <th>Car</th>\n",
       "      <th>Trafficcone</th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>samples/CAM_BACK/n003-2018-01-03-12-03-23+0800...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>samples/CAM_BACK/n003-2018-01-03-12-03-23+0800...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>samples/CAM_BACK/n003-2018-01-03-12-03-23+0800...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>samples/CAM_BACK/n003-2018-01-03-12-03-23+0800...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>samples/CAM_BACK/n003-2018-01-03-12-03-23+0800...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67274</th>\n",
       "      <td>samples/CAM_FRONT_RIGHT/n016-2018-09-10-11-49-...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67275</th>\n",
       "      <td>samples/CAM_FRONT_RIGHT/n016-2018-09-10-11-49-...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67276</th>\n",
       "      <td>samples/CAM_FRONT_RIGHT/n016-2018-09-10-11-49-...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67277</th>\n",
       "      <td>samples/CAM_FRONT_RIGHT/n016-2018-09-10-11-49-...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67278</th>\n",
       "      <td>samples/CAM_FRONT_RIGHT/n016-2018-09-10-11-49-...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67279 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                filename  Human  Barrier  \\\n",
       "0      samples/CAM_BACK/n003-2018-01-03-12-03-23+0800...      1        0   \n",
       "1      samples/CAM_BACK/n003-2018-01-03-12-03-23+0800...      1        0   \n",
       "2      samples/CAM_BACK/n003-2018-01-03-12-03-23+0800...      1        0   \n",
       "3      samples/CAM_BACK/n003-2018-01-03-12-03-23+0800...      1        0   \n",
       "4      samples/CAM_BACK/n003-2018-01-03-12-03-23+0800...      1        0   \n",
       "...                                                  ...    ...      ...   \n",
       "67274  samples/CAM_FRONT_RIGHT/n016-2018-09-10-11-49-...      1        1   \n",
       "67275  samples/CAM_FRONT_RIGHT/n016-2018-09-10-11-49-...      1        0   \n",
       "67276  samples/CAM_FRONT_RIGHT/n016-2018-09-10-11-49-...      1        0   \n",
       "67277  samples/CAM_FRONT_RIGHT/n016-2018-09-10-11-49-...      0        0   \n",
       "67278  samples/CAM_FRONT_RIGHT/n016-2018-09-10-11-49-...      1        0   \n",
       "\n",
       "       Bike  Motorcycle  Truck  Car  Trafficcone  Total  \n",
       "0         0           1      0    1            1      4  \n",
       "1         0           1      1    1            0      4  \n",
       "2         0           0      1    1            0      3  \n",
       "3         0           1      0    1            0      3  \n",
       "4         1           1      0    1            1      5  \n",
       "...     ...         ...    ...  ...          ...    ...  \n",
       "67274     0           0      1    1            0      4  \n",
       "67275     0           0      0    1            0      2  \n",
       "67276     0           1      0    1            0      3  \n",
       "67277     0           0      0    0            1      1  \n",
       "67278     1           0      0    0            0      2  \n",
       "\n",
       "[67279 rows x 9 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_train = df_merging_encoding(object_ann_df_train, sample_data_df_train, category_df_train)\n",
    "labeled_train\n",
    "#print(\"XXX\", object_ann_df_test)\n",
    "#labeled_test = df_merging_encoding(object_ann_df_test, sample_data_df_test, category_df_test)\n",
    "#labeled_val = df_merging_encoding(object_ann_df_val, sample_data_df_val, category_df_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting images that have low quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape pre:  (67279, 9)\n",
      "Shape post:  (67277, 9)\n"
     ]
    }
   ],
   "source": [
    "#use nicks code\n",
    "\n",
    "\n",
    "df = labeled_train\n",
    "print(\"Shape pre: \", df.shape)\n",
    "\n",
    "# Function to calculate average brightness\n",
    "def calculate_brightness(image):\n",
    "    grayscale_image = image.convert(\"L\")\n",
    "    pixel_values = np.array(grayscale_image)\n",
    "    return np.mean(pixel_values)\n",
    "\n",
    "# Function to calculate contrast\n",
    "def calculate_contrast(image):\n",
    "    grayscale_image = image.convert(\"L\")\n",
    "    stat = ImageStat.Stat(grayscale_image)\n",
    "    return stat.stddev[0]\n",
    "\n",
    "# Create a list to store filenames that need to be removed\n",
    "files_to_remove = []\n",
    "\n",
    "# Process each blob\n",
    "blobs = container_client.list_blobs()\n",
    "for blob in blobs:\n",
    "    if blob.name.endswith(\".jpg\") and blob.name.startswith(\"samples/\"):\n",
    "        #print(f\"Processing {blob.name}\")\n",
    "        blob_client = container_client.get_blob_client(blob.name)\n",
    "        blob_data = blob_client.download_blob().readall()\n",
    "        image_data = io.BytesIO(blob_data)\n",
    "        image = Image.open(image_data)\n",
    "\n",
    "        # Calculate brightness and contrast\n",
    "        brightness = calculate_brightness(image)\n",
    "        contrast = calculate_contrast(image)\n",
    "\n",
    "        #print(f\"Brightness: {brightness}, Contrast: {contrast}\")\n",
    "\n",
    "        # Check if the image meets the removal criteria\n",
    "        if brightness < 60 or contrast < 30:\n",
    "            files_to_remove.append(blob.name)\n",
    "\n",
    "# Remove rows from the DataFrame\n",
    "filtered_df = df[~df['filename'].isin(files_to_remove)]\n",
    "print(\"Shape post: \", filtered_df.shape)\n",
    "\n",
    "\n",
    "# Save the filtered DataFrame back to the CSV file\n",
    "filtered_df.to_csv(\"cleaned_preprocessed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addressing imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Status quo of imbalance currently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Human          1.515131\n",
       " Barrier       12.322205\n",
       " Bike           3.686643\n",
       " Motorcycle     4.715092\n",
       " Truck          3.552944\n",
       " Car            1.000000\n",
       " dtype: float64,\n",
       " 4.465335929888663,\n",
       " 12.3222049689441,\n",
       " 0.9176040175824948)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab_no_barrier = labeled_train[[]]\n",
    "\n",
    "# Calculating label counts\n",
    "label_counts = labeled_train.iloc[:, 1:7].sum()\n",
    "max_count = label_counts.max()\n",
    "\n",
    "# Calculating IRLbl\n",
    "ir_lbl = max_count / label_counts\n",
    "\n",
    "# Calculating MeanIR\n",
    "mean_ir = ir_lbl.mean()\n",
    "\n",
    "# Calculating MaxIR\n",
    "max_ir = ir_lbl.max()\n",
    "\n",
    "# Calculating CVIR\n",
    "cv_ir = ir_lbl.std() / mean_ir\n",
    "\n",
    "# Displaying the results\n",
    "ir_lbl, mean_ir, max_ir, cv_ir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coping with imbalance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(labeled_train.shape)\n",
    "\n",
    "# # Normalize the IRLbl to get sampling probabilities inversely proportional to their frequency\n",
    "# sampling_weights = 1 / ir_lbl\n",
    "# sampling_weights /= sampling_weights.sum()\n",
    "\n",
    "# # Calculate how many samples you might want to add to each label\n",
    "# additional_samples_needed = (sampling_weights * 1000000).astype(int)  # scale factor of 1000 for example\n",
    "\n",
    "# # Function to oversample data\n",
    "# def oversample_data(df, label, n_samples):\n",
    "#     # Filter the dataframe for samples containing the specific label\n",
    "#     df_label = df[df[label] == 1]\n",
    "#     # Resample the dataframe\n",
    "#     return df_label.sample(n=n_samples, replace=True)\n",
    "\n",
    "# # Create an empty DataFrame to hold oversampled data\n",
    "# oversampled_data = pd.DataFrame(columns=labeled_train.columns)\n",
    "\n",
    "# # Oversample for each label based on additional samples needed\n",
    "# for label, n_samples in additional_samples_needed.items():\n",
    "#     if n_samples > 0:  # Check if oversampling is needed\n",
    "#         oversampled_data = pd.concat([oversampled_data, oversample_data(labeled_train, label, n_samples)])\n",
    "\n",
    "# # Combine the original DataFrame with the oversampled DataFrame\n",
    "# df_oversampled = pd.concat([labeled_train, oversampled_data]).reset_index(drop=True)\n",
    "\n",
    "# df_oversampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculating label counts\n",
    "# label_counts = df_oversampled.iloc[:, 1:7].sum()\n",
    "# max_count = label_counts.max()\n",
    "\n",
    "# # Calculating IRLbl\n",
    "# ir_lbl = max_count / label_counts\n",
    "\n",
    "# # Calculating MeanIR\n",
    "# mean_ir = ir_lbl.mean()\n",
    "\n",
    "# # Calculating MaxIR\n",
    "# max_ir = ir_lbl.max()\n",
    "\n",
    "# # Calculating CVIR\n",
    "# cv_ir = ir_lbl.std() / mean_ir\n",
    "\n",
    "# # Displaying the results\n",
    "# ir_lbl, mean_ir, max_ir, cv_ir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relative_occurrence = labeled[[\"Human\", \"Barrier\", \"Bike\", \"Motorcycle\", \"Truck\", \"Car\", \"Trafficcone\", \"Total\"]].mean()\n",
    "\n",
    "#relative_occurrence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for potential bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = labeled\n",
    "# columns = ['Human', 'Barrier', 'Bike', 'Motorcycle', 'Truck', 'Car', 'Trafficcone']\n",
    "# results = []\n",
    "\n",
    "# # Nested loops to compute the Chi-squared test for each pair of variables\n",
    "# for col1 in columns:\n",
    "#     for col2 in columns:\n",
    "#         if col1 != col2:\n",
    "#             # Create a contingency table\n",
    "#             contingency_table = pd.crosstab(df[col1], df[col2])\n",
    "#             # Perform the chi-squared test\n",
    "#             chi2, p_value, _, _ = chi2_contingency(contingency_table)\n",
    "#             # Store results\n",
    "#             results.append({'Variable 1': col1, 'Variable 2': col2, 'Chi-squared': chi2, 'p-value': p_value})\n",
    "\n",
    "# # Convert results to DataFrame\n",
    "# result_df = pd.DataFrame(results)\n",
    "# result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A p-value above 0.05, suggests no significant associations and thus no apparent bias or dependency among the variables tested in the dataset.\n",
    "Therefore, only the combination of Car & Truck (and vice versa) shows a dependency which might result in a bias in the models that will be trained based on this data.\n",
    "\n",
    "This fact should be kept in mind when proceeding with the evaluation of the models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data: Load, Normalized, Resize, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_proc(labeled):\n",
    "\n",
    "    shape1 = labeled.shape[0]\n",
    "    print(shape1)\n",
    "    #remove rows with more than 5 labels: too many labels\n",
    "    labeled = labeled[labeled['Total'] < 5]\n",
    "\n",
    "    shape2 = labeled.shape[0]\n",
    "    print(shape2)\n",
    "\n",
    "    removed_rows = shape1 - shape2\n",
    "    print(\"Count of removed rows: \", removed_rows)\n",
    "\n",
    "    labeled = labeled.drop('Total', axis=1)\n",
    "\n",
    "\n",
    "    order_of_labels = [\"Human\", \"Barrier\", \"Bike\", \"Motorcycle\", \"Truck\", \"Car\", \"Trafficcone\"]\n",
    "\n",
    "    #put lables into list\n",
    "    labeled['labels'] = labeled[order_of_labels].values.tolist()\n",
    "\n",
    "    #drop unnecessary columns - stored in array\n",
    "    labeled = labeled.drop(columns=order_of_labels)\n",
    "    return labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33182\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Total'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Total'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-d3b7bb7b9ff4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlabeled_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_proc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabeled_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlabeled_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"labeled_train.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#labeled_test = prepare_proc(labeled_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#labeled_val = prepare_proc(labeled_val)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-89b123f68937>\u001b[0m in \u001b[0;36mprepare_proc\u001b[0;34m(labeled)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#remove rows with more than 5 labels: too many labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mlabeled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabeled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabeled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Total'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mshape2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabeled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3022\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3023\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3024\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3025\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3080\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3082\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3084\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Total'"
     ]
    }
   ],
   "source": [
    "labeled_train = prepare_proc(labeled_train)\n",
    "labeled_train.to_csv(\"labeled_train.csv\", index=False)\n",
    "#labeled_test = prepare_proc(labeled_test)\n",
    "#labeled_val = prepare_proc(labeled_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path):\n",
    "    blob_client = container_client.get_blob_client(path)\n",
    "    blob_data = blob_client.download_blob().readall()  # Directly read all bytes\n",
    "    image = io.BytesIO(blob_data)\n",
    "    return image  #returning the PIL Image object\n",
    "\n",
    "def preprocess(image):\n",
    "    image = Image.open(image)\n",
    "\n",
    "    #convert the PIL image to a NumPy array\n",
    "    image_array = np.array(image)\n",
    "\n",
    "    grayscale_image = cv2.cvtColor(image_array, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    #resize the image\n",
    "    resized_image = cv2.resize(grayscale_image, (256, 256))\n",
    "\n",
    "    #apply histogram equalization to improve contrast\n",
    "    equalized_image_8bit = cv2.equalizeHist(resized_image)\n",
    "\n",
    "    # Re-normalize to [0, 1] range\n",
    "    equalized_normalized_image = equalized_image_8bit / 255.0\n",
    "\n",
    "    #apply histogram equalization\n",
    "    #equalized_image = cv2.equalizeHist(np.uint8(normalized_image * 255))\n",
    "\n",
    "    #apply edge detection using Canny\n",
    "    #edges = cv2.Canny(np.uint8(normalized_image * 255), canny_threshold1, canny_threshold2)\n",
    "\n",
    "    return equalized_normalized_image\n",
    "\n",
    "\n",
    "def preprocessing(path):\n",
    "    image = load_image(path)\n",
    "    preprocessed = preprocess(image)\n",
    "    return preprocessed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up storage\n",
    "connection_string = \"DefaultEndpointsProtocol=https;AccountName=mlfinalexam5505462853;AccountKey=0c40lghglG5/GlNK9yujDQAgo38GKoS2I3DeC/g22hwAEIFANKpmC/TqOpRk4RCT1DbfNiHBFt72+AStB+PfUA==;EndpointSuffix=core.windows.net\"\n",
    "container_name = \"datacomplete\"\n",
    "\n",
    "#create client\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "container_client = blob_service_client.get_container_client(container_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_preprocessing(labeled, purpose):\n",
    "\n",
    "    labeled[\"image\"] = labeled[\"filename\"].apply(preprocessing)\n",
    "    df.drop(\"filename\", axis=1, inplace=True)\n",
    "    json_data = labeled.to_json()\n",
    "\n",
    "    path = 'ALL_'+ purpose + '_labeled_images.json'\n",
    "\n",
    "    # Save JSON data to a file\n",
    "    with open(path, 'w') as file:\n",
    "        file.write(json_data)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_preprocessing(labeled_train, \"train\")\n",
    "#apply_preprocessing(labeled_test, \"test\")\n",
    "#apply_preprocessing(labeled_val, \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
