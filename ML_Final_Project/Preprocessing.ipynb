{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: azure-storage-blob in /opt/anaconda3/lib/python3.8/site-packages (12.19.1)\n",
      "Requirement already satisfied: azure-core<2.0.0,>=1.28.0 in /opt/anaconda3/lib/python3.8/site-packages (from azure-storage-blob) (1.30.1)\n",
      "Requirement already satisfied: cryptography>=2.1.4 in /opt/anaconda3/lib/python3.8/site-packages (from azure-storage-blob) (3.4.7)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/anaconda3/lib/python3.8/site-packages (from azure-storage-blob) (4.11.0)\n",
      "Requirement already satisfied: isodate>=0.6.1 in /opt/anaconda3/lib/python3.8/site-packages (from azure-storage-blob) (0.6.1)\n",
      "Requirement already satisfied: requests>=2.21.0 in /opt/anaconda3/lib/python3.8/site-packages (from azure-core<2.0.0,>=1.28.0->azure-storage-blob) (2.25.1)\n",
      "Requirement already satisfied: six>=1.11.0 in /opt/anaconda3/lib/python3.8/site-packages (from azure-core<2.0.0,>=1.28.0->azure-storage-blob) (1.15.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/anaconda3/lib/python3.8/site-packages (from cryptography>=2.1.4->azure-storage-blob) (1.14.5)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/lib/python3.8/site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob) (2.20)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/anaconda3/lib/python3.8/site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/lib/python3.8/site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.8/site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (2023.7.22)\n",
      "\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.8/site-packages (1.6.2)\n",
      "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /opt/anaconda3/lib/python3.8/site-packages (from scipy) (1.22.2)\n",
      "\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.8/site-packages (0.24.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn) (1.22.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn) (1.6.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn) (2.1.0)\n",
      "\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: opencv-python in /opt/anaconda3/lib/python3.8/site-packages (4.9.0.80)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /opt/anaconda3/lib/python3.8/site-packages (from opencv-python) (1.22.2)\n",
      "\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install azure-storage-blob\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import io  # Used to convert bytes to a file-like object\n",
    "import pandas as pd\n",
    "import re\n",
    "%pip install scipy\n",
    "%pip install scikit-learn\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "%pip install opencv-python\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up storage for reading meta data\n",
    "connection_string = \"DefaultEndpointsProtocol=https;AccountName=mlfinalexam5505462853;AccountKey=0c40lghglG5/GlNK9yujDQAgo38GKoS2I3DeC/g22hwAEIFANKpmC/TqOpRk4RCT1DbfNiHBFt72+AStB+PfUA==;EndpointSuffix=core.windows.net\"\n",
    "container_name = \"publicdata\"\n",
    "\n",
    "#create client\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "container_client = blob_service_client.get_container_client(container_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all blobs and process JSON files\n",
    "json_data = {}\n",
    "\n",
    "blobs = container_client.list_blobs()\n",
    "for blob in blobs:\n",
    "    if blob.name.endswith(\".json\"):\n",
    "        # Get the blob content\n",
    "        blob_client = container_client.get_blob_client(blob.name)\n",
    "        blob_content = blob_client.download_blob().content_as_text()\n",
    "\n",
    "        # Convert to JSON and add to the dictionary using the blob's name as the key\n",
    "        json_data[blob.name] = json.loads(blob_content)  # Now storing JSON content as a dictionary\n",
    "\n",
    "# #Assigning specific JSON data to variables\n",
    "# category = json_data.get(\"v1.0-mini/category.json\", [])\n",
    "# sensor = json_data.get(\"v1.0-mini/sensor.json\", {})\n",
    "# surface_ann = json_data.get(\"v1.0-mini/surface_ann.json\", {})\n",
    "# attribute = json_data.get(\"v1.0-mini/attribute.json\", {})\n",
    "# log = json_data.get(\"v1.0-mini/log.json\", {})\n",
    "# calibrated_sensor = json_data.get(\"v1.0-mini/calibrated_sensor.json\", {})\n",
    "# sample_data = json_data.get(\"v1.0-mini/sample_data.json\", {})\n",
    "# sample = json_data.get(\"v1.0-mini/sample.json\", {})\n",
    "# ego_pose = json_data.get(\"v1.0-mini/ego_pose.json\", {})\n",
    "# object_ann = json_data.get(\"v1.0-mini/object_ann.json\", {})\n",
    "\n",
    "default_path_train = \"all-metadata/v1.0-train/\"\n",
    "default_path_test = \"all-metadata/v1.0-test/\"\n",
    "default_path_val = \"all-metadata/v1.0-val/\"\n",
    "\n",
    "\n",
    "def load_jsons(default_path):\n",
    "    object_ann = json_data.get(default_path + \"object_ann.json\", {})\n",
    "    sample_data = json_data.get(default_path + \"sample_data.json\", {})\n",
    "    category = json_data.get(default_path + \"category.json\", [])\n",
    "\n",
    "    return object_ann, sample_data, category\n",
    "\n",
    "object_ann_train, sample_data_train, category_train = load_jsons(default_path_train)\n",
    "#object_ann_test, sample_data_test, category_test = load_jsons(default_path_test)\n",
    "#object_ann_val, sample_data_val, category_val = load_jsons(default_path_val)\n",
    "#object_ann_test, sample_data_test, category_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Category Clustering and Label Encodeing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to dataframe for better processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_df(object_ann, sample_data, category):\n",
    "    #turn into proper json\n",
    "    object_ann_json = json.dumps(object_ann, indent=4)\n",
    "    sample_data_json = json.dumps(sample_data, indent=4)\n",
    "    category_json = json.dumps(category, indent=4)\n",
    "    #turn into dataframe\n",
    "    object_ann_df = pd.read_json(object_ann_json)\n",
    "    category_df = pd.read_json(category_json)\n",
    "    sample_data_df = pd.read_json(sample_data_json)\n",
    "    return object_ann_df, sample_data_df, category_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_ann_df_train, sample_data_df_train, category_df_train = json_to_df(object_ann_train, sample_data_train, category_train)\n",
    "#object_ann_df_test, sample_data_df_test, category_df_test = json_to_df(object_ann_test, sample_data_test, category_test)\n",
    "#object_ann_df_val, sample_data_df_val, category_df_val = json_to_df(object_ann_val, sample_data_val, category_val)\n",
    "#object_ann_df_test.shape, sample_data_df_test.shape, category_df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster chosen Categories into right \"Parent\" - category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clustering of categories\n",
    "def transform_category(category_str):\n",
    "    if re.match(r'^human\\.', category_str):\n",
    "        return 'Human'\n",
    "    if re.match(r'^movable_object\\.barrier', category_str):\n",
    "        return 'Barrier'\n",
    "    if re.match(r'^movable_object\\.cone', category_str):\n",
    "        return 'Cone'\n",
    "    if re.match(r'^vehicle\\.bicycle', category_str):\n",
    "        return 'Bike'\n",
    "    if re.match(r'^vehicle\\.motorcycle', category_str):\n",
    "        return 'Motorcycle'\n",
    "    if re.match(r'^vehicle\\.truck', category_str):\n",
    "        return 'Truck'\n",
    "    if re.match(r'^vehicle\\.car', category_str):\n",
    "        return 'Car'\n",
    "    if re.match(r'^movable_object\\.trafficcone', category_str):\n",
    "        return 'Trafficcone'\n",
    "    return None\n",
    "\n",
    "def category_proc(category_df):\n",
    "    #apply function to the category column -> clustering of chosen categories\n",
    "    category_df['name'] = category_df[\"name\"].apply(transform_category)\n",
    "\n",
    "    #remove description\n",
    "    category_df = category_df.drop(columns=\"description\")\n",
    "\n",
    "    #drop rows that are not included in the chosen categories\n",
    "    category_df = category_df.dropna(subset=['name'])\n",
    "    return category_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_df_train = category_proc(category_df_train)\n",
    "#category_df_test = category_proc(category_df_test)\n",
    "#category_df_val = category_proc(category_df_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Dataframes based on Data model and FK-PK dependencies & proper encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def df_merging_encoding(object_ann_df, sample_data_df, category_df):\n",
    "\n",
    "    label_data_v2 = None\n",
    "\n",
    "    #merge dataframes based on foreign keys to connect labeling with image data\n",
    "    obj_cat = pd.merge(object_ann_df, category_df, left_on='category_token', right_on='token', how='inner')\n",
    "    obj_cat = obj_cat.dropna(subset=['name'])\n",
    "\n",
    "    #remove list of columns from dataframe\n",
    "    columns_to_remove = [\"token_x\", \"token_y\", \"category_token\", \"bbox\", \"mask\", \"attribute_tokens\", ]\n",
    "    obj_cat = obj_cat.drop(columns=columns_to_remove)\n",
    "\n",
    "    #filter so that only key frames are included\n",
    "    print(sample_data_df.info())\n",
    "    sample_data_df = sample_data_df[sample_data_df[\"is_key_frame\"] == True]\n",
    "\n",
    "    #merge with sample data\n",
    "    label_data_v1 = pd.merge(sample_data_df, obj_cat, left_on='token', right_on='sample_data_token', how='left')\n",
    "    label_data_v1['name'] = label_data_v1['name'].fillna('empty')\n",
    "    #group to see all labels for each image\n",
    "    label_data_v1 = label_data_v1.groupby(\"filename\")[\"name\"].value_counts().rename(\"count\").reset_index()\n",
    "\n",
    "\n",
    "    #turn labels from names to label_data_v1 variables\n",
    "    dummies = pd.get_dummies(label_data_v1['name'])\n",
    "\n",
    "    # Join the dummy variables to the main dataframe\n",
    "    labeled = pd.concat([label_data_v1, dummies], axis=1)\n",
    "    labeled[dummies.columns] = labeled[dummies.columns].astype(int)\n",
    "\n",
    "    # Drop the original 'name' column\n",
    "    labeled = labeled.drop(['name'], axis=1)\n",
    "    labeled = labeled.drop(['empty'], axis=1)\n",
    "    labeled = labeled.drop(['count'], axis=1)\n",
    "\n",
    "    # Group by 'filename' and aggregate the data\n",
    "    labeled = labeled.groupby('filename').agg({\n",
    "        'Human': 'max',\n",
    "        'Barrier': 'max',\n",
    "        'Bike': 'max',\n",
    "        'Motorcycle': 'max',\n",
    "        'Truck': 'max',\n",
    "        'Car': 'max',\n",
    "        'Trafficcone': 'max'\n",
    "    }).reset_index()\n",
    "\n",
    "    labeled['Total'] = labeled[[\"Human\", \"Barrier\", \"Bike\", \"Motorcycle\", \"Truck\", \"Car\", \"Trafficcone\"]].sum(axis=1)\n",
    "    #labeled['NoDetec'] = labeled[[\"Human\", \"Barrier\", \"Bike\", \"Motorcycle\", \"Truck\", \"Car\", \"Trafficcone\"]].apply(lambda row: 1 if (row == 0).all() else 0, axis=1)\n",
    "\n",
    "    return labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 872181 entries, 0 to 872180\n",
      "Data columns (total 12 columns):\n",
      " #   Column                   Non-Null Count   Dtype         \n",
      "---  ------                   --------------   -----         \n",
      " 0   token                    872181 non-null  object        \n",
      " 1   sample_token             872181 non-null  object        \n",
      " 2   ego_pose_token           872181 non-null  object        \n",
      " 3   calibrated_sensor_token  872181 non-null  object        \n",
      " 4   filename                 872181 non-null  object        \n",
      " 5   fileformat               872181 non-null  object        \n",
      " 6   width                    872181 non-null  int64         \n",
      " 7   height                   872181 non-null  int64         \n",
      " 8   timestamp                872181 non-null  datetime64[ns]\n",
      " 9   is_key_frame             872181 non-null  bool          \n",
      " 10  prev                     872181 non-null  object        \n",
      " 11  next                     872181 non-null  object        \n",
      "dtypes: bool(1), datetime64[ns](1), int64(2), object(8)\n",
      "memory usage: 74.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "labeled_train = df_merging_encoding(object_ann_df_train, sample_data_df_train, category_df_train)\n",
    "#print(\"XXX\", object_ann_df_test)\n",
    "#labeled_test = df_merging_encoding(object_ann_df_test, sample_data_df_test, category_df_test)\n",
    "#labeled_val = df_merging_encoding(object_ann_df_val, sample_data_df_val, category_df_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relative_occurrence = labeled[[\"Human\", \"Barrier\", \"Bike\", \"Motorcycle\", \"Truck\", \"Car\", \"Trafficcone\", \"Total\"]].mean()\n",
    "\n",
    "#relative_occurrence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for potential bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = labeled\n",
    "# columns = ['Human', 'Barrier', 'Bike', 'Motorcycle', 'Truck', 'Car', 'Trafficcone']\n",
    "# results = []\n",
    "\n",
    "# # Nested loops to compute the Chi-squared test for each pair of variables\n",
    "# for col1 in columns:\n",
    "#     for col2 in columns:\n",
    "#         if col1 != col2:\n",
    "#             # Create a contingency table\n",
    "#             contingency_table = pd.crosstab(df[col1], df[col2])\n",
    "#             # Perform the chi-squared test\n",
    "#             chi2, p_value, _, _ = chi2_contingency(contingency_table)\n",
    "#             # Store results\n",
    "#             results.append({'Variable 1': col1, 'Variable 2': col2, 'Chi-squared': chi2, 'p-value': p_value})\n",
    "\n",
    "# # Convert results to DataFrame\n",
    "# result_df = pd.DataFrame(results)\n",
    "# result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A p-value above 0.05, suggests no significant associations and thus no apparent bias or dependency among the variables tested in the dataset.\n",
    "Therefore, only the combination of Car & Truck (and vice versa) shows a dependency which might result in a bias in the models that will be trained based on this data.\n",
    "\n",
    "This fact should be kept in mind when proceeding with the evaluation of the models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data: Load, Normalized, Resize, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_proc(labeled):\n",
    "    labeled = labeled.drop('Total', axis=1)\n",
    "\n",
    "\n",
    "    order_of_labels = [\"Human\", \"Barrier\", \"Bike\", \"Motorcycle\", \"Truck\", \"Car\", \"Trafficcone\"]\n",
    "\n",
    "    #put lables into list\n",
    "    labeled['labels'] = labeled[order_of_labels].values.tolist()\n",
    "\n",
    "    #drop unnecessary columns - stored in array\n",
    "    labeled = labeled.drop(columns=order_of_labels)\n",
    "    return labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_train = prepare_proc(labeled_train)\n",
    "labeled_train.to_csv(\"labeled_train.csv\", index=False)\n",
    "#labeled_test = prepare_proc(labeled_test)\n",
    "#labeled_val = prepare_proc(labeled_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path):\n",
    "    blob_client = container_client.get_blob_client(path)\n",
    "    blob_data = blob_client.download_blob().readall()  # Directly read all bytes\n",
    "    image = io.BytesIO(blob_data)\n",
    "    return image  #returning the PIL Image object\n",
    "\n",
    "def preprocess(image):\n",
    "    image = Image.open(image)\n",
    "\n",
    "    #convert the PIL image to a NumPy array\n",
    "    image_array = np.array(image)\n",
    "\n",
    "    grayscale_image = cv2.cvtColor(image_array, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    #resize the image\n",
    "    resized_image = cv2.resize(grayscale_image, (256, 256))\n",
    "\n",
    "    #apply histogram equalization to improve contrast\n",
    "    equalized_image_8bit = cv2.equalizeHist(resized_image)\n",
    "\n",
    "    # Re-normalize to [0, 1] range\n",
    "    equalized_normalized_image = equalized_image_8bit / 255.0\n",
    "\n",
    "    #apply histogram equalization\n",
    "    #equalized_image = cv2.equalizeHist(np.uint8(normalized_image * 255))\n",
    "\n",
    "    #apply edge detection using Canny\n",
    "    #edges = cv2.Canny(np.uint8(normalized_image * 255), canny_threshold1, canny_threshold2)\n",
    "\n",
    "    return equalized_normalized_image\n",
    "\n",
    "\n",
    "def preprocessing(path):\n",
    "    image = load_image(path)\n",
    "    preprocessed = preprocess(image)\n",
    "    return preprocessed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up storage\n",
    "connection_string = \"DefaultEndpointsProtocol=https;AccountName=mlfinalexam5505462853;AccountKey=0c40lghglG5/GlNK9yujDQAgo38GKoS2I3DeC/g22hwAEIFANKpmC/TqOpRk4RCT1DbfNiHBFt72+AStB+PfUA==;EndpointSuffix=core.windows.net\"\n",
    "container_name = \"datacomplete\"\n",
    "\n",
    "#create client\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "container_client = blob_service_client.get_container_client(container_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_preprocessing(labeled, purpose):\n",
    "\n",
    "    labeled[\"image\"] = labeled[\"filename\"].apply(preprocessing)\n",
    "    df.drop(\"filename\", axis=1, inplace=True)\n",
    "    json_data = labeled.to_json()\n",
    "\n",
    "    path = 'ALL_'+ purpose + '_labeled_images.json'\n",
    "\n",
    "    # Save JSON data to a file\n",
    "    with open(path, 'w') as file:\n",
    "        file.write(json_data)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_preprocessing(labeled_train, \"train\")\n",
    "#apply_preprocessing(labeled_test, \"test\")\n",
    "#apply_preprocessing(labeled_val, \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
